{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import api\n",
    "import random\n",
    "import movement_viz as v\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import util\n",
    "\n",
    "def init_q_table():\n",
    "    '''\n",
    "    init q table (initilizations are all [0])\n",
    "    defines grid as 40x40, 4 possible actions (N, S, E, W)\n",
    "    access grid as row, col, action\n",
    "    ex of indexing: q-tab[0][0][0] gird 0:0, action 'N'\n",
    "    '''\n",
    "\n",
    "    return (np.zeros((40, 40, 4)))\n",
    "\n",
    "def num_to_move(num):\n",
    "    '''\n",
    "    translates the index returned from np.argmax()\n",
    "    when accessing our representation of the q-table\n",
    "    structure into the expexted value that the API \n",
    "    can understand\n",
    "    '''\n",
    "    if num == 0:\n",
    "        return 'N'\n",
    "    elif num == 1:\n",
    "        return 'S'\n",
    "    elif num == 2:\n",
    "        return 'E'\n",
    "    elif num == 3:\n",
    "        return 'W'\n",
    "\n",
    "    return 'ERROR!'\n",
    "\n",
    "def update_q_table(location, q_table, reward, gamma, new_loc, learning_rate, move_num):\n",
    "    '''\n",
    "    bellman eq: NEW Q(s,a) = Q(s,a) + learning_rate * [R(s,a) + gamma * maxQ'(s',a') - Q(s,a)]\n",
    "    '''\n",
    "\n",
    "    #collecting the current understanding of the best q value based upon our new location, weight it by gamma and add reward\n",
    "    right_side = reward + gamma * q_table[new_loc[0], new_loc[1], :].max() - q_table[location[0], location[1], move_num]\n",
    "\n",
    "    #use the previous location to \n",
    "    new_q = q_table[location[0], location[1], move_num] + learning_rate * right_side\n",
    "\n",
    "    #update q_table with new value\n",
    "    q_table[location[0], location[1], move_num] = new_q\n",
    "\n",
    "\n",
    "def learn(q_table, worldId=0, mode='train', learning_rate=0.001, gamma=0.9, epsilon=0.9, good_term_states=[], bad_term_states=[], epoch=0, obstacles=[], run_num=0, verbose=True):\n",
    "    '''\n",
    "    ~MAIN LEARNING FUNCTION~\n",
    "    takes in:\n",
    "    -the Q-table data structure (numpy 3-dimensional array)\n",
    "    -worldID (for api and plotting)\n",
    "    -mode (train or exploit)\n",
    "    -learning rate (affects q-table calculation)\n",
    "    -gamma (weighting of the rewards)\n",
    "    -epsilon (determines the amount of random exploration the agen does)\n",
    "    -good_term_states\n",
    "    -bad_term_states\n",
    "    -eposh\n",
    "    -run number\n",
    "    -verbosity\n",
    "    returns: q_table [NumPy Array], good_term_states [list], bad_term_states [list], obstacles [list]\n",
    "    '''\n",
    "\n",
    "    #create the api instance\n",
    "    a = api.API(worldId=worldId)\n",
    "    w_res = a.enter_world()\n",
    "\n",
    "\n",
    "    if verbose: print(\"w_res: \",w_res)\n",
    "\n",
    "\n",
    "    #init terminal state reached\n",
    "    terminal_state = False\n",
    "\n",
    "    #create a var to track the type of terminal state\n",
    "    good = False\n",
    "\n",
    "    #accumulate the rewards so far for plotting reward over step\n",
    "    rewards_acquired = []\n",
    "\n",
    "    #find out where we are\n",
    "    loc_response = a.locate_me()\n",
    "\n",
    "    #create a list of everywhere we've been for the viz\n",
    "    visited = []\n",
    "\n",
    "    if verbose: print(\"loc_response\",loc_response)\n",
    "    \n",
    "    #OK response looks like {\"code\":\"OK\",\"world\":\"0\",\"state\":\"0:2\"}\n",
    "    if loc_response[\"code\"] != \"OK\":\n",
    "            print(f\"something broke on locate_me call \\nresponse lookes like: {loc_response}\")\n",
    "            return -1\n",
    "    \n",
    "    # convert JSON into a tuple (x,y)\n",
    "    location = int(loc_response[\"state\"].split(':')[0]), int(loc_response[\"state\"].split(':')[1]) #location is a tuple (x, y)\n",
    "    \n",
    "    # SET UP FIGURE FOR VISUALIZATION.\n",
    "    pyplot.figure(1, figsize=(10,10))\n",
    "    curr_board = [[float('-inf')] * 40 for temp in range(40)]\n",
    "    \n",
    "    #keep track of where we've been for the visualization\n",
    "    visited.append(location)\n",
    "    while True:\n",
    "        #////////////////// CODE FOR VISUALIZATION\n",
    "        curr_board[location[1]][location[0]] = 1\n",
    "        for i in range (len(curr_board)):\n",
    "            for j in range(len(curr_board)):\n",
    "                if (curr_board[i][j] != 0):\n",
    "                    curr_board[i][j] -= .1\n",
    "        for obstacle in obstacles:\n",
    "            if obstacle in visited:\n",
    "                obstacles.remove(obstacle)\n",
    "        v.update_grid(curr_board, good_term_states, bad_term_states, obstacles, run_num, epoch, worldId, location, verbose)\n",
    "        #//////////////// END CODE FOR VISUALIZATION\n",
    "\n",
    "        #in q-table, get index of best option for movement based on our current state in the world\n",
    "        if mode == 'train':\n",
    "            #use an episolon greedy approach to randomly explore or exploit\n",
    "            if np.random.uniform() < epsilon:\n",
    "                unexplored = np.where(q_table[location[0]][location[1]].astype(int) == 0)[0]\n",
    "                explored = np.where(q_table[location[0]][location[1]].astype(int) != 0)[0]\n",
    "\n",
    "                if unexplored.size != 0:\n",
    "                    move_num = int( np.random.choice( unexplored ) )\n",
    "                else:\n",
    "                    move_num = int( np.random.choice( explored ) )\n",
    "            else:\n",
    "                move_num = np.argmax(q_table[location[0]][location[1]])\n",
    "\n",
    "        else:\n",
    "            #mode is exploit -we'll use what we already have in the q-table to decide on our moves\n",
    "            move_num = np.argmax(q_table[location[0]][location[1]])\n",
    "\n",
    "        #make the move - transition into a new state\n",
    "        move_response = a.make_move(move=num_to_move(move_num), worldId=str(worldId)) \n",
    "\n",
    "        if verbose: print(\"move_response\", move_response)\n",
    "        #OK response looks like {\"code\":\"OK\",\"worldId\":0,\"runId\":\"931\",\"reward\":-0.1000000000,\"scoreIncrement\":-0.0800000000,\"newState\":{\"x\":\"0\",\"y\":3}}\n",
    "        \n",
    "\n",
    "        if move_response[\"code\"] != \"OK\":\n",
    "            #handel the unexpected\n",
    "            print(f\"something broke on make_move call \\nresponse lookes like: {move_response}\")\n",
    "\n",
    "            move_failed = True\n",
    "            while move_failed:\n",
    "                move_response = a.make_move(move=num_to_move(move_num), worldId=str(worldId))\n",
    "\n",
    "                print(\"\\n\\ntrying move again!!\\n\\n\")\n",
    "\n",
    "                if move_response[\"code\"] == 'OK':\n",
    "                    move_failed = False\n",
    "        \n",
    "        # check that we're not in a terminal state, and if not convert new location JSON into tuple\n",
    "        if move_response[\"newState\"] is not None:\n",
    "            #we're now in new_loc, which will be a tuple of where we are according to the API\n",
    "            #KEEP IN MIND the movment of our agent is apparently STOCHASTIC\n",
    "            new_loc = int(move_response[\"newState\"][\"x\"]), int(move_response[\"newState\"][\"y\"]) #tuple (x,y)\n",
    "            \n",
    "            # keep track of if we hit any obstacles\n",
    "            expected_loc = list(location)\n",
    "\n",
    "            #convert the move we tried to make into an expected location where we think we'll end up (expected_loc) \n",
    "            recent_move = num_to_move(move_num)\n",
    "      \n",
    "            if recent_move == \"N\":\n",
    "                expected_loc[1]+=1\n",
    "            elif recent_move == \"S\":\n",
    "                expected_loc[1]-=1\n",
    "            elif recent_move == \"E\":\n",
    "                expected_loc[0]+=1\n",
    "            elif recent_move == \"W\":\n",
    "                expected_loc[0]-=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            expected_loc = tuple(expected_loc)\n",
    "\n",
    "            if verbose: print(f\"New Loc: {new_loc} (where we actually are now):\")\n",
    "            if verbose: print(f\"Expected Loc: {expected_loc} (where we thought we were going to be):\")\n",
    "\n",
    "            if (mode == \"train\"):\n",
    "                obstacles.append(expected_loc)\n",
    "\n",
    "            #continue to track where we have been\n",
    "            visited.append(new_loc)\n",
    "\n",
    "            #if we placed an obstacle there in the vis, remove it\n",
    "            for obstacle in obstacles:\n",
    "                if obstacle in visited:\n",
    "                    obstacles.remove(obstacle)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #we hit a terminal state\n",
    "            terminal_state = True\n",
    "            print(\"\\n\\n--------------------------\\nTERMINAL STATE ENCOUNTERED\\n--------------------------\\n\\n\")\n",
    "       \n",
    "        #get the reward for the most recent move we made\n",
    "        reward = float(move_response[\"reward\"])\n",
    "\n",
    "\n",
    "        #add reward to plot\n",
    "        rewards_acquired.append(reward) \n",
    "\n",
    "        #if we are training the model then update the q-table for the state we were in before\n",
    "        #using the bellman-human algorithim\n",
    "        if mode == \"train\":\n",
    "            update_q_table(location, q_table, reward, gamma, new_loc, learning_rate, move_num)\n",
    "        \n",
    "        #update our current location variable to our now current location\n",
    "        location = new_loc\n",
    "\n",
    "\n",
    "        #if we are in a terminal state then we need to collect the information for our visualization\n",
    "        #and we need to end our current training epoch\n",
    "        if terminal_state:\n",
    "            print(f\"Terminal State REWARD: {reward}\")\n",
    "\n",
    "            if reward > 0:\n",
    "                #we hit a positive reward so keep track of it as a good reward terminal-state\n",
    "                good = True\n",
    "            if not(location in good_term_states) and not(location in bad_term_states):\n",
    "                #update our accounting of good and bad terminal states for the visualization\n",
    "                if good:\n",
    "                    good_term_states.append(location)\n",
    "                else:\n",
    "                    bad_term_states.append(location)\n",
    "\n",
    "            #update our visualization a last time before moving onto the next epoch\n",
    "            v.update_grid(curr_board, good_term_states, bad_term_states, obstacles, run_num, epoch, worldId, location, verbose)\n",
    "            break\n",
    "\n",
    "    #possibly not needed but this seperates out the plot\n",
    "    pyplot.figure(2, figsize=(5,5))\n",
    "    #cumulative average for plotting reward by step over time purposes\n",
    "    cumulative_average = np.cumsum(rewards_acquired) / (np.arange(len(rewards_acquired)) + 1)\n",
    "    # plot reward over each step of the agent\n",
    "    utils.plot_learning(worldId, epoch, cumulative_average, run_num)\n",
    "\n",
    "    return q_table, good_term_states, bad_term_states, obstacles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
